{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a865db1",
   "metadata": {},
   "source": [
    "<h2>Processing the Modelling Data</h2>\n",
    "<p>This file is concerned with processing and cleaning the ACS data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7af9e7",
   "metadata": {},
   "source": [
    "<h3>Initial Data Processing</h3>\n",
    "<p>All datasets can be initially processed to remove unnecessary columns and convert columns to numeric</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f378d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dfs(dfs_raw):\n",
    "    \n",
    "    for df in dfs_raw:\n",
    "        # drop the first row of every dataset\n",
    "        df.drop(index = df.index[0], axis = 0, inplace = True)\n",
    "        # strip GEO_ID of first 11 characters\n",
    "        df.loc[:,'GEO_ID'] = df.loc[:, 'GEO_ID'].str[-11:]\n",
    "        # drop NAME column\n",
    "        df.drop(\"NAME\", axis=1, inplace=True)\n",
    "        # drop all margin of error columns\n",
    "        df.drop(list(df.filter(regex = 'M')), axis = 1, inplace = True)\n",
    "        # convert non-GEO_ID columns to numeric\n",
    "        num_cols = df.columns.drop('GEO_ID')\n",
    "        df[num_cols] = df[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        # drop all NA values\n",
    "        df.dropna(inplace= True)\n",
    "    \n",
    "    return dfs_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda0c39",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Race Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsRace(df):\n",
    "    acs_race = df.copy()\n",
    "    acs_race.rename(columns={'GEO_ID':'geoid',\n",
    "                             'B02001_001E':'total', \n",
    "                             'B02001_002E':'white',\n",
    "                             'B02001_003E':'black',\n",
    "                             'B02001_004E':'nativeam',\n",
    "                             'B02001_005E':'asian', \n",
    "                             'B02001_006E':'pacislander'}, inplace=True)\n",
    "    \n",
    "    acs_race['other'] = acs_race['B02001_007E'] + acs_race['B02001_008E']\n",
    "    acs_race.drop(['B02001_007E', \n",
    "                   'B02001_008E', \n",
    "                   'B02001_009E', \n",
    "                   'B02001_010E'], axis=1, inplace=True)\n",
    "    \n",
    "    return acs_race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9386c",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Age Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsAge(df):\n",
    "    acs_age = df.copy()\n",
    "    acs_age.drop(['B01001_002E', \n",
    "                   'B01001_026E'], axis=1, inplace=True)\n",
    "    acs_age.rename(columns={'GEO_ID':'geoid',\n",
    "                             'B01001_001E':'total'}, inplace=True)\n",
    "    \n",
    "    acs_age['children'] = acs_age['B01001_003E'] + acs_age['B01001_004E'] + acs_age['B01001_005E'] + acs_age['B01001_006E'] + acs_age['B01001_027E'] + acs_age['B01001_028E'] + acs_age['B01001_029E'] + acs_age['B01001_030E']\n",
    "    acs_age.drop(['B01001_003E', \n",
    "                  'B01001_004E', \n",
    "                  'B01001_005E', \n",
    "                  'B01001_006E', \n",
    "                  'B01001_027E', \n",
    "                  'B01001_028E', \n",
    "                  'B01001_029E', \n",
    "                  'B01001_030E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_age['youngAdult'] = acs_age['B01001_007E'] + acs_age['B01001_008E'] + acs_age['B01001_009E'] + acs_age['B01001_010E'] + acs_age['B01001_031E'] + acs_age['B01001_032E'] + acs_age['B01001_033E'] + acs_age['B01001_034E']\n",
    "    acs_age.drop(['B01001_007E', \n",
    "                  'B01001_008E', \n",
    "                  'B01001_009E', \n",
    "                  'B01001_010E', \n",
    "                  'B01001_031E', \n",
    "                  'B01001_032E', \n",
    "                  'B01001_033E', \n",
    "                  'B01001_034E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_age['adult'] = acs_age['B01001_011E'] + acs_age['B01001_012E'] + acs_age['B01001_013E'] + acs_age['B01001_014E'] + acs_age['B01001_015E'] + acs_age['B01001_016E'] + acs_age['B01001_017E'] + acs_age['B01001_018E'] + acs_age['B01001_019E'] + acs_age['B01001_035E'] + acs_age['B01001_036E'] + acs_age['B01001_037E'] + acs_age['B01001_038E'] + acs_age['B01001_039E'] + acs_age['B01001_040E'] + acs_age['B01001_041E'] + acs_age['B01001_042E'] + acs_age['B01001_043E']\n",
    "    acs_age.drop(['B01001_011E', \n",
    "                  'B01001_012E', \n",
    "                  'B01001_013E', \n",
    "                  'B01001_014E', \n",
    "                  'B01001_015E', \n",
    "                  'B01001_016E', \n",
    "                  'B01001_017E', \n",
    "                  'B01001_018E',\n",
    "                  'B01001_019E', \n",
    "                  'B01001_035E', \n",
    "                  'B01001_036E', \n",
    "                  'B01001_037E', \n",
    "                  'B01001_038E', \n",
    "                  'B01001_039E', \n",
    "                  'B01001_040E', \n",
    "                  'B01001_041E', \n",
    "                  'B01001_042E',\n",
    "                  'B01001_043E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_age['elderly'] = acs_age['B01001_020E'] + acs_age['B01001_021E'] + acs_age['B01001_022E'] + acs_age['B01001_023E'] + acs_age['B01001_024E'] + acs_age['B01001_025E'] + acs_age['B01001_044E'] + acs_age['B01001_045E'] + acs_age['B01001_046E'] + acs_age['B01001_047E'] + acs_age['B01001_048E'] + acs_age['B01001_049E']\n",
    "    acs_age.drop(['B01001_020E', \n",
    "                  'B01001_021E',  \n",
    "                  'B01001_022E', \n",
    "                  'B01001_023E', \n",
    "                  'B01001_024E', \n",
    "                  'B01001_025E', \n",
    "                  'B01001_044E', \n",
    "                  'B01001_045E', \n",
    "                  'B01001_046E', \n",
    "                  'B01001_047E', \n",
    "                  'B01001_048E', \n",
    "                  'B01001_049E'], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    return acs_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b70d49",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Tenure Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsTenure(df):\n",
    "    \n",
    "    acs_tenure = df.copy()\n",
    "    acs_tenure.rename(columns={'GEO_ID':'geoid', \n",
    "                               'B25003_001E':'total', \n",
    "                               'B25003_002E':'ownerOcc', \n",
    "                               'B25003_003E':'renterOcc'}, inplace=True)\n",
    "    \n",
    "    return acs_tenure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b398b",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Hours Worked Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc85f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsHoursWk(df):\n",
    "    \n",
    "    acs_hoursWk = df.copy()\n",
    "    acs_hoursWk.drop(['B23020_002E', \n",
    "                  'B23020_003E'], axis=1, inplace=True)\n",
    "    acs_hoursWk.rename(columns={'GEO_ID':'geoid', \n",
    "                                'B23020_001E':'hoursWk'}, inplace=True)\n",
    "    \n",
    "    return acs_hoursWk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2708313",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Citizenship Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f177d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsCitizen(df):\n",
    "    \n",
    "    acs_citizen = df.copy()\n",
    "    acs_citizen.rename(columns={'GEO_ID':'geoid', \n",
    "                                'B05001_001E':'total',\n",
    "                                'B05001_002E':'citizenUS',\n",
    "                                'B05001_003E':'citizenPR',\n",
    "                                'B05001_004E':'citizenAbroad',\n",
    "                                'B05001_005E':'citizenNaturalized',\n",
    "                                'B05001_006E':'notCitizen'}, inplace=True)\n",
    "    \n",
    "    return acs_citizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e31c2",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Marital Status Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb316615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsMarriage(df):\n",
    "    \n",
    "    acs_marriage = df.copy()\n",
    "    acs_marriage.drop(['B12001_002E', \n",
    "                      'B12001_011E',\n",
    "                      'B12001_004E',\n",
    "                      'B12001_006E',\n",
    "                      'B12001_013E',\n",
    "                      'B12001_015E'], axis=1, inplace=True)\n",
    "    acs_marriage.rename(columns={'GEO_ID':'geoid', \n",
    "                                'B12001_001E':'total'}, inplace=True)\n",
    "    \n",
    "    acs_marriage['single'] = acs_marriage['B12001_003E'] + acs_marriage['B12001_012E']\n",
    "    acs_marriage.drop(['B12001_003E', 'B12001_012E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_marriage['married'] = acs_marriage['B12001_005E'] + acs_marriage['B12001_014E']\n",
    "    acs_marriage.drop(['B12001_005E', 'B12001_014E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_marriage['separated'] = acs_marriage['B12001_007E'] + acs_marriage['B12001_016E']\n",
    "    acs_marriage.drop(['B12001_007E', 'B12001_016E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_marriage['marriedOther'] = acs_marriage['B12001_008E'] + acs_marriage['B12001_017E']\n",
    "    acs_marriage.drop(['B12001_008E', 'B12001_017E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_marriage['widowed'] = acs_marriage['B12001_009E'] + acs_marriage['B12001_018E']\n",
    "    acs_marriage.drop(['B12001_009E', 'B12001_018E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_marriage['divorced'] = acs_marriage['B12001_010E'] + acs_marriage['B12001_019E']\n",
    "    acs_marriage.drop(['B12001_010E', 'B12001_019E'], axis=1, inplace=True)\n",
    "    \n",
    "    return acs_marriage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2aa044",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Transportation Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsTrans(df):\n",
    "    \n",
    "    acs_trans = df.copy()\n",
    "    acs_trans.drop(['B08301_002E', \n",
    "                    'B08301_010E',\n",
    "                    'B08301_004E'], axis=1, inplace=True)\n",
    "    acs_trans.rename(columns={'GEO_ID':'geoid', \n",
    "                              'B08301_001E':'total', \n",
    "                              'B08301_011E':'bus', \n",
    "                              'B08301_014E':'train', \n",
    "                              'B08301_015E':'ferry', \n",
    "                              'B08301_016E':'taxi', \n",
    "                              'B08301_018E':'bicycle', \n",
    "                              'B08301_019E':'walked', \n",
    "                              'B08301_020E':'otherTravel', \n",
    "                              'B08301_021E':'homeworker'}, inplace=True)\n",
    "    \n",
    "    acs_trans['privateVehicle'] = acs_trans['B08301_003E'] + acs_trans['B08301_017E']\n",
    "    acs_trans.drop(['B08301_003E', 'B08301_017E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_trans['carpool'] = acs_trans['B08301_005E'] + acs_trans['B08301_006E'] + acs_trans['B08301_007E'] + acs_trans['B08301_008E'] + acs_trans['B08301_009E']\n",
    "    acs_trans.drop(['B08301_005E', 'B08301_006E', 'B08301_007E', 'B08301_008E', 'B08301_009E'], axis=1, inplace=True)\n",
    "\n",
    "    acs_trans['subwayTram'] = acs_trans['B08301_012E'] + acs_trans['B08301_013E']\n",
    "    acs_trans.drop(['B08301_012E', 'B08301_013E'], axis=1, inplace=True)\n",
    "    \n",
    "    return acs_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec968700",
   "metadata": {},
   "source": [
    "<h3>Cleaning Structure Build Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsYearBuilt_2010(df):\n",
    "    \n",
    "    acs_yearBuilt = df.copy()\n",
    "    \n",
    "    acs_yearBuilt.rename(columns={'GEO_ID':'geoid', \n",
    "                                  'B25034_001E':'total',  \n",
    "                                  'B25034_004E':'1990_1999', \n",
    "                                  'B25034_005E':'1980_1989', \n",
    "                                  'B25034_006E':'1970_1979', \n",
    "                                  'B25034_007E':'1960_1969', \n",
    "                                  'B25034_008E':'1950_1959', \n",
    "                                  'B25034_009E':'1940_1949',\n",
    "                                  'B25034_010E':'pre_1939'}, inplace=True)\n",
    "    \n",
    "    acs_yearBuilt['2000_later'] = acs_yearBuilt['B25034_002E'] + acs_yearBuilt['B25034_003E']\n",
    "    acs_yearBuilt.drop(['B25034_002E', 'B25034_003E'], axis=1, inplace=True)\n",
    "    \n",
    "    return acs_yearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsYearBuilt_2019(df):\n",
    "    \n",
    "    acs_yearBuilt = df.copy()\n",
    "    \n",
    "    acs_yearBuilt.rename(columns={'GEO_ID':'geoid', \n",
    "                                  'B25034_001E':'total',   \n",
    "                                  'B25034_005E':'1990_1999', \n",
    "                                  'B25034_006E':'1980_1989', \n",
    "                                  'B25034_007E':'1970_1979', \n",
    "                                  'B25034_008E':'1960_1969', \n",
    "                                  'B25034_009E':'1950_1959', \n",
    "                                  'B25034_010E':'1940_1949',\n",
    "                                  'B25034_011E':'pre_1939'}, inplace=True)\n",
    "    \n",
    "    acs_yearBuilt['2000_later'] = acs_yearBuilt['B25034_002E'] + acs_yearBuilt['B25034_003E'] + acs_yearBuilt['B25034_004E']\n",
    "    acs_yearBuilt.drop(['B25034_002E', 'B25034_003E', 'B25034_004E'], axis=1, inplace=True)\n",
    "    \n",
    "    return acs_yearBuilt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbece71",
   "metadata": {},
   "source": [
    "<h3>Cleaning Units in Structure Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acsStructure(df):\n",
    "    \n",
    "    acs_structure = df.copy()\n",
    "    \n",
    "    acs_structure.rename(columns={'GEO_ID':'geoid', \n",
    "                                  'B25024_001E':'total', \n",
    "                                  'B25024_002E':'detatched_1', \n",
    "                                  'B25024_003E':'attached_1', \n",
    "                                  'B25024_004E':'2_unit', \n",
    "                                  'B25024_005E':'3to4_unit', \n",
    "                                  'B25024_006E':'5to9_unit', \n",
    "                                  'B25024_007E':'10to19_unit', \n",
    "                                  'B25024_008E':'20to49_unit', \n",
    "                                  'B25024_009E':'50plus_unit'}, inplace=True)\n",
    "    \n",
    "    acs_structure['mobile'] = acs_structure['B25024_010E'] + acs_structure['B25024_011E']\n",
    "    acs_structure.drop(['B25024_010E', 'B25024_011E'], axis=1, inplace=True)\n",
    "    \n",
    "    return acs_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e706b0",
   "metadata": {},
   "source": [
    "<h3>Convert Necessary Values to Shares</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd400e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toShare(df):\n",
    "    pred_data = pd.DataFrame()\n",
    "    for n in df.columns.tolist():\n",
    "        #print(\"Converting \" + n)\n",
    "        if n == 'geoid':\n",
    "            pred_data.loc[:,n] = df.loc[:,'geoid']\n",
    "        else:\n",
    "            pred_data.loc[:,n] = (df.loc[:,n] / df.loc[:,'total'])\n",
    "            \n",
    "    pred_data.drop(['total'], axis=1, inplace=True)\n",
    "    \n",
    "    return pred_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacb715",
   "metadata": {},
   "source": [
    "<h3>Creating the Dataset</h3>\n",
    "<p>Read in the data, clean each dataset individually, include certain datasets in the convert to percentages (or convert to shares) process, merge all datasets together by common geo_id column</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df_list, year):\n",
    "    \n",
    "    # Perform initial cleaning for all data\n",
    "    print(\"perform initial processing...\")\n",
    "    clean_dfs = process_dfs(dataframes_list)\n",
    "    \n",
    "    # Clean each dataset appropriately\n",
    "    acs_race = clean_acsRace(clean_dfs[0])\n",
    "    print(\"cleaning race data...\")\n",
    "\n",
    "    acs_age = clean_acsAge(clean_dfs[1])\n",
    "    print(\"cleaning age data...\")\n",
    "\n",
    "    acs_tenure = clean_acsTenure(clean_dfs[2])\n",
    "    print(\"cleaning tenure data...\")\n",
    "\n",
    "    acs_hoursWk = clean_acsHoursWk(clean_dfs[3])\n",
    "    print(\"cleaning hours worked data...\")\n",
    "\n",
    "    acs_citizen = clean_acsCitizen(clean_dfs[4])\n",
    "    print(\"cleaning citizenship data...\")\n",
    "\n",
    "    acs_marriage = clean_acsMarriage(clean_dfs[5])\n",
    "    print(\"cleaning marriage data...\")\n",
    "\n",
    "    acs_trans = clean_acsTrans(clean_dfs[6])\n",
    "    print(\"cleaning transportation data...\")\n",
    "    \n",
    "    if year == 2010:\n",
    "        acs_yearBuilt = clean_acsYearBuilt_2010(clean_dfs[7])\n",
    "        print(\"cleaning year built data...\")\n",
    "    else:\n",
    "        acs_yearBuilt = clean_acsYearBuilt_2019(clean_dfs[7])\n",
    "        print(\"cleaning year built data...\")\n",
    "    \n",
    "    acs_structure = clean_acsStructure(clean_dfs[8])\n",
    "    print(\"cleaning structure data...\")\n",
    "\n",
    "    # Convert necessary datasets to shares\n",
    "    to_share = [acs_race, acs_age, acs_tenure, acs_citizen, acs_marriage, acs_trans, acs_yearBuilt, acs_structure]\n",
    "    percents = []\n",
    "    \n",
    "    print(\"create shares...\")\n",
    "    for df in to_share:\n",
    "        temp = df.copy()\n",
    "        percents.append(toShare(temp))\n",
    "    \n",
    "    # Combine all datasets into one\n",
    "    print(\"merging datasets...\")\n",
    "    df_all = percents + [acs_hoursWk]\n",
    "    df_merged = reduce(lambda x,y: pd.merge(x,y, on='geoid', how='outer'), df_all)   \n",
    "    \n",
    "    df_merged.set_index('geoid', inplace=True)\n",
    "    df_merged.fillna(0, inplace=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8342215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the main function\n",
    "\n",
    "# Process the 2010 Data\n",
    "print(\"processing 2010 data...\")\n",
    "list_of_names = ['ACSDT5Y2010.B02001', #0 Race\n",
    "                 'ACSDT5Y2010.B01001', #1 Age\n",
    "                 'ACSDT5Y2010.B25003', #2 Tenure\n",
    "                 'ACSDT5Y2010.B23020', #3 Hours worked\n",
    "                 'ACSDT5Y2010.B05001', #4 Citizenship\n",
    "                 'ACSDT5Y2010.B12001', #5 Marital Status\n",
    "                 'ACSDT5Y2010.B08301', #6 Transportation\n",
    "                 'ACSDT5Y2010.B25034', #7 Year Structure Built\n",
    "                 'ACSDT5Y2010.B25024'] #8 Units in Structure\n",
    "# create empty list\n",
    "dataframes_list = []\n",
    " \n",
    "# append datasets into the list\n",
    "for i in range(len(list_of_names)):\n",
    "    temp_df = pd.read_csv(\"/Users/ritalaplante/Desktop/Thesis Data and Analytics/01-Raw Data/ACS_2010/\"+list_of_names[i]+\".csv\")\n",
    "    dataframes_list.append(temp_df)\n",
    "    \n",
    "data_2010 = process_data(dataframes_list, 2010)\n",
    "print(\"dataset creation complete...\\n\")\n",
    "\n",
    "# Process the 2019 Data\n",
    "print(\"processing 2019 data...\")\n",
    "list_of_names = ['ACSDT5Y2019.B02001', #0 Race\n",
    "                 'ACSDT5Y2019.B01001', #1 Age\n",
    "                 'ACSDT5Y2019.B25003', #2 Tenure\n",
    "                 'ACSDT5Y2019.B23020', #3 Hours worked\n",
    "                 'ACSDT5Y2019.B05001', #4 Citizenship\n",
    "                 'ACSDT5Y2019.B12001', #5 Marital Status\n",
    "                 'ACSDT5Y2019.B08301', #6 Transportation\n",
    "                 'ACSDT5Y2019.B25034', #7 Year Structure Built\n",
    "                 'ACSDT5Y2019.B25024'] #8 Units in Structure\n",
    "# create empty list\n",
    "dataframes_list = []\n",
    " \n",
    "# append datasets into the list\n",
    "for i in range(len(list_of_names)):\n",
    "    temp_df = pd.read_csv(\"/Users/ritalaplante/Desktop/Thesis Data and Analytics/01-Raw Data/ACS_2019/\"+list_of_names[i]+\".csv\")\n",
    "    dataframes_list.append(temp_df)\n",
    "    \n",
    "data_2019 = process_data(dataframes_list, 2019)\n",
    "print(\"dataset creation complete...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3eeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c69690",
   "metadata": {},
   "outputs": [],
   "source": [
    "s19 = data_2019.columns\n",
    "s10 = data_2010.columns\n",
    "print(\"2019 variables diff'd against 2010 variables: \" + str(s19.difference(s10)))\n",
    "print(\"2010 variables diff'd against 2019 variables: \" + str(s10.difference(s19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('/Users/ritalaplante/Desktop/Senior Thesis/Data/outputs', exist_ok=True)  \n",
    "data_2010.to_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/02-Cleaned Predictor Data/predictor2010.csv', index = True)  \n",
    "data_2019.to_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/02-Cleaned Predictor Data/predictor2019.csv', index = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b8b81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
