{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7081ebe",
   "metadata": {},
   "source": [
    "<h1>Transform and Rescale</h1>\n",
    "<p>This notebook focusses on transforming and scaling the predictor data to prevent issues of skewed data from biasing results in later analysis. This notebook produces two datasets that can be used and reloaded in the Random Forests and this file should only be re-run if the transformation used for the scoring data changes.</p>\n",
    "<p>Robust standardization using median and Interquartile Range</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb44ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import decomposition  \n",
    "from sklearn.preprocessing import scale  \n",
    "from sklearn import preprocessing \n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# For reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "r_state = 42\n",
    "random.seed(r_state) \n",
    "np.random.seed(r_state)\n",
    "\n",
    "# Needed on a Mac\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42262093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_status_scores(dtype):\n",
    "    status = pd.read_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/04-Neighborhood Scores/scores' + to_use + '.csv', index_col=0)  # SES scores\n",
    "    \n",
    "    status.dropna(inplace = True)\n",
    "    \n",
    "    # Scores\n",
    "    status.drop(['RANK_10','RANK_19'], axis=1, inplace=True)\n",
    "    status.rename(columns={\n",
    "        'SES_10':'SES 2010',\n",
    "        'SES_19':'SES 2019',\n",
    "        'SES_ASC':'SES Ascent 2010-2019',\n",
    "        'SES_PR_10':'SES 2010 Percentile', # 99 = High-status\n",
    "        'SES_PR_19':'SES 2019 Percentile', # 99 = High-status\n",
    "        'SES_PR_ASC':'SES Percentile Ascent 2010-2019'\n",
    "    }, inplace=True)\n",
    "    return status\n",
    "\n",
    "def load_predictors(dtype):\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cf058",
   "metadata": {},
   "source": [
    "<h2>Choose Your Transformation</h2>\n",
    "<p>It should be easy to load and reload data once the transformation is changed. Use same transformation that was applied in the scoring notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe36910",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_use = 'Untransformed' # Options are: ['Untransformed','Box-Cox','Log']\n",
    "\n",
    "SES = load_status_scores(to_use)  # SES scores in 2011\n",
    "\n",
    "d10input = pd.read_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/04-Neighborhood Scores/inputs2010' + to_use + '.csv', index_col=0)  # SES inputs\n",
    "d19input = pd.read_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/04-Neighborhood Scores/inputs2019' + to_use + '.csv', index_col=0)  # SES inputs\n",
    "\n",
    "# Rename to remove confusion\n",
    "d10input.rename(columns=lambda x: re.sub(' 2010','',x), inplace=True)\n",
    "d19input.rename(columns=lambda x: re.sub(' 2019','',x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2783f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Read in processed datasets\n",
    "d10 = pd.read_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/02-Cleaned Predictor Data/predictor2010.csv', index_col=0)  #  Main dataset for 2001\n",
    "d19 = pd.read_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/02-Cleaned Predictor Data/predictor2019.csv', index_col=0)  #  Main dataset for 2011\n",
    "\n",
    "d10 = pd.merge(d10input, d10, how='inner', left_index=True, right_index=True)\n",
    "d19 = pd.merge(d19input, d19, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Have \" + str(len(d10.columns)+1) + \" variables to work with.\")\n",
    "d10.sample(3, random_state=r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "s10 = set(d10.columns)\n",
    "s19 = set(d19.columns)\n",
    "print(\"2010 vs 2019 variable check: \" + str(s10.difference(s19)))\n",
    "print(\"2010 vs 2009 variable check: \" + str(s19.difference(s10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f361a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SES.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives = pd.DataFrame()\n",
    "for c in d10.columns:\n",
    "    descriptives = descriptives.append(pd.concat([d10[c].describe(),d19[c].describe()],axis=0,ignore_index=True),ignore_index=False)\n",
    "\n",
    "descriptives.columns = ['2010 Count','2010 Mean','2010 StD','2010 Min','2010 LQ','2010 Median','2010 UQ','2010 Max',\n",
    "                        '2019 Count','2019 Mean','2019 StD','2019 Min','2019 LQ','2019 Median','2019 UQ','2019 Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77204698",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8762294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This enables to re-use the same sample below\n",
    "dsample = descriptives.sample(4, random_state=r_state).index.values\n",
    "dsample = np.append(dsample,\n",
    "                    ['geoid','House Prices',\n",
    "                     'Percentage with Bach Degree','Percentage Professional Workers',\n",
    "                     'Household Income', 'Contract Rent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a368ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives[descriptives.index.isin(dsample)][\n",
    "    ['2010 Min','2019 Min','2010 Max','2019 Max','2010 Median','2019 Median']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives.to_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/10-Summary Stats/Full Dataset ' + to_use + ' Descriptives.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e2fa0",
   "metadata": {},
   "source": [
    "<h2>Rescaling Data</h2>\n",
    "<p>The code below uses unit variance scaling on the 2010 and 2019 data. Both datasets are centered independently using median-removal.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393da48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust scaling _without_ centering\n",
    "# and _with_ common scaling. We do this \n",
    "# because 2010 and 2019 won't have the \n",
    "# same centre but we do want them to use\n",
    "# a common scale.\n",
    "rs1 = preprocessing.RobustScaler(with_centering=False, quantile_range=(25.0,75.0))\n",
    "\n",
    "#  Train on 2010 data set\n",
    "rs1.fit(d10)\n",
    "\n",
    "# Apply the same unit variance scaling to both years\n",
    "d10_trs1 = pd.DataFrame(data=rs1.transform(d10), index=d10.index, columns=d10.columns)\n",
    "d19_trs1 = pd.DataFrame(data=rs1.transform(d19), index=d19.index, columns=d19.columns)\n",
    "\n",
    "# Create new robust scaler for centering \n",
    "# _without_ common scaling.\n",
    "rs2 = preprocessing.RobustScaler(with_scaling=False)  \n",
    "\n",
    "# Centre independently\n",
    "d10_trs2 = pd.DataFrame(data=rs2.fit_transform(d10_trs1), index=d10.index, columns=d10.columns)  \n",
    "d19_trs2 = pd.DataFrame(data=rs2.fit_transform(d19_trs1), index=d19.index, columns=d19.columns)\n",
    "\n",
    "d10_trs2 = pd.merge(d10_trs2, SES, how='inner', left_index=True, right_index=True)\n",
    "d10_trs2.drop(['SES 2010','SES 2019', 'SES Ascent 2010-2019', 'SES 2010 Percentile', 'SES 2019 Percentile', 'SES Percentile Ascent 2010-2019'], axis=1, inplace=True)\n",
    "\n",
    "d19_trs2 = pd.merge(d19_trs2, SES, how='inner', left_index=True, right_index=True)\n",
    "d19_trs2.drop(['SES 2010','SES 2019', 'SES Ascent 2010-2019', 'SES 2010 Percentile', 'SES 2019 Percentile', 'SES Percentile Ascent 2010-2019'], axis=1, inplace=True)\n",
    "\n",
    "#  Write the transformed data to csv\n",
    "d10_trs2.to_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/05-Transformed and Scaled Data/TransformedAndScaled2010' + to_use + '.csv', index=True)\n",
    "d19_trs2.to_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/05-Transformed and Scaled Data/TransformedAndScaled2019' + to_use + '.csv', index=True) \n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30190c94",
   "metadata": {},
   "source": [
    "<h2>Check that We Did Everything Correctly</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf648ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives_trs1 = pd.DataFrame()\n",
    "for c in d10_trs1.columns:\n",
    "    descriptives_trs1 = descriptives_trs1.append(pd.concat([d10_trs1[c].describe(),d19_trs1[c].describe()],axis=0,ignore_index=True),ignore_index=False)\n",
    "\n",
    "descriptives_trs1.columns = ['2010 Count','2010 Mean','2010 StD','2010 Min','2010 LQ','2010 Median','2010 UQ','2010 Max',\n",
    "                             '2019 Count','2019 Mean','2019 StD','2019 Min','2019 LQ','2019 Median','2019 UQ','2019 Max']\n",
    "\n",
    "# Useful, but time-consuming\n",
    "#plot_checks(d01_trs1, dsample, 'First-transform')\n",
    "\n",
    "descriptives_trs1[descriptives_trs1.index.isin(dsample)][\n",
    "    ['2010 Min','2019 Min','2010 Max','2019 Max','2010 Median','2019 Median','2010 Mean','2019 Mean']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b935112",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives_trs2 = pd.DataFrame()\n",
    "for c in d10_trs2.columns:\n",
    "    descriptives_trs2 = descriptives_trs2.append(pd.concat([d10_trs2[c].describe(),d19_trs2[c].describe()],axis=0,ignore_index=True),ignore_index=False)\n",
    "\n",
    "descriptives_trs2.columns = ['2010 Count','2010 Mean','2010 StD','2010 Min','2010 LQ','2010 Median','2010 UQ','2010 Max',\n",
    "                             '2019 Count','2019 Mean','2019 StD','2019 Min','2019 LQ','2019 Median','2019 UQ','2019 Max']\n",
    "\n",
    "# Useful, but time-consuming\n",
    "#plot_checks(d01_trs2, dsample, 'Second-transform')\n",
    "\n",
    "descriptives_trs2[descriptives_trs2.index.isin(dsample)][\n",
    "    ['2010 Min','2019 Min','2010 Max','2019 Max','2010 Median','2019 Median','2010 Mean','2019 Mean']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives_trs2.to_csv('/Users/ritalaplante/Desktop/Thesis Data and Analytics/10-Summary Stats/Full Dataset Transformed ' + to_use + ' Descriptives.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673baa49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
